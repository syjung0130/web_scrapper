
# 3. Crawling
페이지 하나만 분석하는 것은 스크레이퍼라고 부르고 여러 페이지, 여러 사이트를 이동하는 스크레이퍼를 크롤러라고 부른다.
웹크롤러라는 이름은 이름 그대로 웹을 크롤링한다는 의미.. 특정 URL에서 페이지를 가져오고, 그 페이지를 검사해서 다른 URL을 찾고, 다시 그 페이지를 가져오는 작업을 반복한다.

# 3.1 단일 도메인 내의 이동
임의의 위키백과 페이지를 가져와서 페이지의 링크 목록을 가져오는 코드를 보자


```python
from urllib2 import urlopen
from bs4 import BeautifulSoup

html = urlopen("https://en.wikipedia.org/wiki/Kevin_Bacon")
bsObj = BeautifulSoup(html, "html.parser")

for link in bsObj.findAll("a"):
    if 'href' in link.attrs:
        print(link.attrs['href'])
```

이 링크에는 '항목링크'와 '항목링크가 아닌 다른 링크'들이 섞여있다.
항목링크가 다른 링크들과 구분되고 항목링크들만이 가진 공통점으로 아래 세가지가 있다.
 - 이 링크들은 id가 bodyContent인 div안에 있다.
 - URL에는 세미콜론이 포함되어있지 않다.
 - URL은 /wiki/로 시작한다.


```python
from urllib2 import urlopen
from bs4 import BeautifulSoup
import re

html = urlopen("https://en.wikipedia.org/wiki/Kevin_Bacon")
bsObj = BeautifulSoup(html, "html.parser")

for link in bsObj.find("div", {"id":"bodyContent"}).findAll("a", href=re.compile("^(/wiki/)((?!:).)*$")):
    if 'href' in link.attrs:
        print(link.attrs['href'])
```

아래 예제는 "https://en.wikipedia.org/wiki/Kevin_Bacon" 페이지의 항목링크들을 저장한 뒤에 각 링크들의 항목링크들을 출력하는 예제이다.


```python
from urllib2 import urlopen
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())

def getLinks(articleUrl):
    html = urlopen("https://en.wikipedia.org"+articleUrl)
    bsObj = BeautifulSoup(html, "html.parser")
    return bsObj.find("div", {"id":"bodyContent"}).findAll("a", href=re.compile("^(/wiki/)((?!:).)*$"))

links = getLinks("/wiki/Kevin_Bacon")
print('@@@@ links type: {0}'.format(type(links)))
while len(links) > 0:
    newArticle = links[random.randint(0, len(links)-1)].attrs["href"]
    print(newArticle)
    links = getLinks(newArticle)
```
